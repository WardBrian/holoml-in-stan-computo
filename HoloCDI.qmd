---
title: "Estimation and Sampling for Holographic Coherent Diffraction Imaging"
subtitle: "Low-photon phase retrieval with MLE, MAP, VB, and MCMC"
author:
  - name: Brian Ward
    corresponding: true
    email: bward@flatironinstitute.org
    url: https://brianward.dev/
    orcid: 0000-0002-9841-3342
    affiliations:
      - name: Flatiron Institute
        department: Center for Computational Mathematics
        url: https://www.simonsfoundation.org/flatiron/center-for-computational-mathematics/
  - name: Bob Carpenter
    email: bcarpenter@flatironinstitute.org
    url: https://bob-carpenter.github.io/
    orcid: 0000-0002-2433-9688
    affiliations:
      - name: Flatiron Institute
        department: Center for Computational Mathematics
        url: https://www.simonsfoundation.org/flatiron/center-for-computational-mathematics/
  - name: David Barmherzig
    email: dbarmherzig@flatironinstitute.org
    url: https://davidbar.org/
    orcid: 0000-0003-2466-981X
    affiliations:
      - name: Flatiron Institute
        department: Center for Computational Mathematics
        url: https://www.simonsfoundation.org/flatiron/center-for-computational-mathematics/
date: last-modified
date-modified: last-modified
description: |
#  This case study surveys several inference and evaluation schemes for holographic coherent diffraction imaging.
abstract: >+
  This case study provides a forward statistical model for holographic coherent diffraction imaging (Holo CDI) that generates a random photon flux from an image and explores several ways of solving the inverse problem (i.e., recovering the image from the measured flux). Maximum a posteriori estimates (MAP) and penalized maximum likelihood estimates (MLE) are provided by limited memory quasi-Newton optimization (L-BFGS).  Variational Bayes (VB) is implemented with Pathfinder.  Markov chain Monte Carlo (MCMC) is implemented with the no-U-turn sampler (NUTS).  Evaluations include log density, root mean square error (RMSE), and the image-specific measures structural similarity (SSIM) and and peak signal-to-noise ratio (PSNR).
keywords: [Stan, coherent-diffraction-imaging, statistics, bayesian, phase-retrieval, inverse-problem <br /> <br /> ]
citation:
  type: article-journal
  container-title: "Computo"
  doi: "xxxx"
  url: https://computo.sfds.asso.fr/template-computo-quarto
  publisher: "Société Française de Statistique"
  issn: "2824-7795"
bibliography: references.bib
github-user: WardBrian
repo: "holoml-in-stan-computo"
draft: true # set to false once the build is running
published: false # will be set to true once accepted
monofont: Menlo
monofontoptions: 
  - Scale=0.80
format:
  computo-html: default
  computo-pdf: default
jupyter: python3
---


# Introduction

Coherent diffraction imaging (CDI) is a technique for imaging nanoscale biomolecules such as macroviruses and proteins.  CDI involves exposing the object being imaged to a coherent beam of X-rays and measuring the diffracted photon flux at a sensor placed behind the object being imaged.  The diffraction pattern theoretically follows a Fourier transform, but only the squared magnitude is observed, which induces a phase-retrieval problem where the real and imaginary components of the complex variables must be inferred so that the process may be inverted to form an image.  Solving the inverse problem for CDI is highly challenging and typically lacks a unique solution [@barnett2020].

Holographic coherent diffraction imaging (HCDI) is a variant of CDI in which the specimen is placed some distance away from a known reference object, and the data observed is the pattern of  diffraction around both the specimen and the reference. The addition of a reference object reduces the inverse problem to a linear deconvolution problem which has a unique, closed-form solution in the idealized setting [@barmherzig2019].

In this note, we follow @barmherzig2022 in defining a forward statistical model that, given the image being reconstructed, defines the expected photon flux at the sensors in terms of the Fourier transform of the image and Poisson sampling of incident photons.  Starting from the model of @barmherzig2022, we add an intrinsic conditional autoregressive (ICAR) prior on the image which can be used to control smoothing of adjacent pixels [@besag1991].  We then log-odds (logit) transform the parameters of the model, which represent pixel intensity between 0 and 1, into unconstrained variables so the model has support on all of $\mathbb{R}^{M \times N}$ for an $M \times N$ pixel image. 

The forward model of holographic CDI defines a sampling distribution for the observed photon flux.  When combined with a prior, the forward model induces an inverse problem whereby we measure photon flux on a grid of sensors and infer the image that caused it.  We will explore the following standard algorithms for exact and approximate solution of the inverse problem.

* Penalized maxmum likelihood (MLE):  Solve with optimization, do not adjust for unconstraining transform.
* Maximum a posteriori (MAP): Solve with optimization, adjusting for the unconstraining transform.
* Variational Bayes (VB): Solve by minimizing Kullback-Leibler divergence from an approximate posterior to the true posterior and importance resampling.
* Markov chain Monte Carlo (MCMC): Solve by sampling from the posterior and averaging.

Computationally, we will code the models using Stan [@carpenter2017], a probabilistic programming language for expressing differentiable log densities.  All that is needed to solve the inverse problem with Stan is a program implementing the forward model's log density.  Stan uses derivative-based methods for optimization, sampling, and variational inference in order to scale with dimension.  It also contains a comprehensive suite of posterior analysis tools for summarizing fits to data.  

@barmherzig2022 used two different optimizers for maximum likelihood inference, conjugate gradient and trust regions. We will employ limited memory quasi-Newton optimization in the form of the limited-memory BFGS (L-BFGS) optimizer [@zhu1997], which tends to outperform both conjugate gradient and trust-region methods for relatively simple optimization problems in moderately high dimensions like this one.  If the model adjusts for the inverse log odds (logistic) transform, the optimizer produces a maximum a posteriori (MAP) estimate, whereas if the model does not adjust for the change of variables, the optimizer produces a penalized maximum likelihood estimate (MLE).

We employ two different Bayesian methods for solving the inverse problem, variational Bayes (VB) and Markov chain Monte Carlo (MCMC) sampling.  For MCMC, we employ the no-U-turn sampler (NUTS) [@hoffman2014], which is an adaptive form of Hamiltonian Monte Carlo (HMC).  With MCMC, the result is an exact posterior sampler for which estimation error goes to zero as sample size goes to infinity.  For variational Bayes, we employ Pathfinder [@zhang2022].  With VB, the results are only approximate, and in particular, posterior uncertainty tends to be systematically underestimated because we minimize KL-divergence from the approximation to the target distribution.

## Python boilerplate

Generating the data requires the standard Python numerical libraries `scipy.stats` and `numpy`, as well as the plotting library `matplotlib` to display results.  Stan is accessed through the `cmdstanpy` and `bridgestan` interfaces.  There is also code for converting RGB-coded images to grayscale.

```{python}
import cmdstanpy as csp
import bridgestan as bs
import numpy as np
import scipy as s
import os
import matplotlib as mpl
import matplotlib.image as mpimg
import matplotlib.pyplot as plt

# disable axes drawing, since we are showing images
mpl.rc('axes.spines', top=False, bottom=False, left=False, right=False)
mpl.rc('axes', facecolor='white')
mpl.rc("xtick", bottom=False, labelbottom=False)
mpl.rc("ytick", left=False, labelleft=False)

def rgb2gray(rgb):
    """Convert a nxmx3 RGB array to a grayscale nxm array.

    This function uses the same internal coefficients as MATLAB:
    https://www.mathworks.com/help/matlab/ref/rgb2gray.html
    """
    r, g, b = rgb[:, :, 0], rgb[:, :, 1], rgb[:, :, 2]
    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b

    return gray
```

# Test images

For simplicity of exposition, we will restrict attention to black and white digital images at $256 \times 256$ pixel resolution.  The pixel values will be represented with continuous values in $[0, 1]$, with $0$ being pure black and $1$ pure white. An image is thus an element $X \in [0, 1]^{256 \times 256}.$  Compute time to evaluate the log density and gradients scales linearly in time and memory for each image dimension, which means it scales quadratically in $N$ for an $N \times N$ image.

## Primary test image

We will concentrate on the target image of a [mimivirus](https://en.wikipedia.org/wiki/Mimivirus), a type of [giant virus](https://en.wikipedia.org/wiki/Giant_virus). It is giant in the sense that its genome is over one million base pairs in length.  The mimivirus is structured as an icosahedral capsid of approximately 400nm with filaments extending another 100nm around the capsid.  An image is shown in @fig-mimivirus.

```{python}
#| fig-cap: Image of a mimivirus.  The capsid is an icosahedron 400nm across with 100nm filaments. Image copyright 2008 by E. Ghigo J. Kartenbeck, P. Lien, L. Pelkmans, C. Capo, J.L. Mege, and D. Raoult D. 2008, and distributed under the [CC BY 2.5](https://creativecommons.org/licenses/by/2.5) license.
#| label: fig-mimivirus
#| fig-align: center
#| fig-pos: 't'

X_src = rgb2gray(mpimg.imread('mimivirus.png'))
plt.imshow(X_src, cmap='gray', vmin=0, vmax=1)
```

## Secondary test images

In addition to the mimivirus image, we will also simulate and reconstruct the ten images show in @fig-test_img.

```{python}
#| fig-cap: Images form the USC/SIPI test repository.  First row, left to right) female [Bell Labs?], house, tree, jelly beans, moon surface.  Second row, left to right) aerial, airplane, clock, resolution chart, chemical plant.
#| label: fig-test_img
#| fig-align: center
#| fig-pos: 't'

img_dir = 'img/usc-sipi'
image_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])
fig, axs = plt.subplots(2, 5, figsize=(20, 8))
axs = axs.ravel()
for i, ax in enumerate(axs):
    img_path = os.path.join(img_dir, image_files[i])
    gray_img = mpimg.imread(img_path)
    ax.imshow(gray_img, cmap='gray')
plt.tight_layout()
plt.show()
```


# The generative model

The generative model here will generate images, represented with a random variable $X \in \mathbb{R}^{N \times N}$ from the prior and then generate photon counts ($Y \in \mathbb{N}^{M_1 \times M_2}$) conditioned on the images.  The counts are wider because they include photon counts for the image, the separator, the reference image, and the padding.

The generation of the observations proceeds by generating an image from the prior,
\begin{equation}
x \sim p_X\,(\cdot).
\end{equation}
Then, given the image $x$, generate the photons observed at the sensors $y$ conditoned on the image $x$ using the sampling distribution,
\begin{equation}
y \sim p_{Y \mid X}\,(\cdot \mid x).
\end{equation}

## The prior

Because the space $[0, 1]^{256 \times 256}$ has finite hypervolume, it would be possible to use a uniform prior over pixel values.  At the other extreme, it would also be possible to introduce a meaningful natural image prior [@kadkhodaie2021], a promising modern approach to which is based on diffusion models [@graikos2022,@fei2023].

Instead of a uniform prior or natural image prior, we will use a simple intrinsic conditional autoregressive (ICAR) prior [@besag1991], which penalizes differences between adjacent pixels according to a scale $\sigma > 0.$ These priors were originally developed for image denoising. The ICAR prior is defined by
\begin{equation}
p_X(x) \propto \prod_{(m, n) \sim (m', n')} \textrm{normal}(x_{m,n} \mid x_{m',n'}, \ \sigma),
\end{equation}
where $\sim$ is the adjacency relationship defined to be unique so that  $(m, n) \sim (m + 1, n)$ for $m < 256$ and $(m, n) \sim (m, n + 1)$ for $n < 256$ and where $\sigma$ is the scale of the normal distribution.  This distribution can, in theory, be normalized within the space $[0, 1]^{256 \times 256},$ but that is not going to be necessary for any of the inference methods we use.  It would also be possible to smooth along the diagonals by including an adjacency relation $(m, n) \sim (m + 1, n + 1)$ for $m, n < 256,$ but we do not do that here.  

An alternative parameterization that we do not consider here would place a prior directly on the unconstrained log odds of the pixel values, with independent standard logistic distributions producing uniform distributions when transformed back to $(0, 1)$; the boundaries cannot be included in the transform as they wold correspond to $\pm \infty$ on the unconstrained scale.

## The sampling distribution

The sampling distribution proceeds in two stages.  First, the image is combined with a separator and a reference image and then padded.  The result defines the shape of the sensor grid.  A discrete Fourier transform is applied to the padded and separated image and reference, the squared absolute values of which determine the proportion of photons that are expected to arrive at the specified sensor.  Second, the expected proportions are normalized for the intensity of the beam and then the photons are generated according to a simple Poisson process.

### The reference image

For the experiments here, we use an idealized optimal reference image $R$, the uniformly redundant array (URA) reference [@fenimore1978].  The pattern used here is known as a *uniformly redundant array* (URA) [@fenimore1978].  The URA is depicted in @fig-ura.  The URA has been shown to be an optimal reference image for this kind of work.   We have just included the reference array as a file, but it can also be generated programatically using a Python package such as [cappy](https://github.com/bpops/cappy).   The URA is black and white (i.e., no intermediate gray values), and it has the same dimensions ($256 \times 256$) as the image being inferred. It will be supplied as data, so that other references can be swapped in depending on the actual reference image being used (e.g., a square cutout, a pinhole, or no reference at all).

```{python}
#| fig-cap: Uniformly redundant array. This reference image is optimal for identification through phase retrieval. Other reference images may be used with the same code by swapping the reference data file.
#| label: fig-ura
#| fig-align: center
#| fig-pos: 't'

R = np.loadtxt('URA.csv', delimiter=",", dtype=int)
plt.imshow(R, cmap='gray')
```

### The seperator and padding

There is also a purely black separator of the same size as the image, which we will use $Z$ to denote because it is full of zero values.  Like the reference pattern, this separator is typically chosen to be the same size as the specimen.  The image $X$ will be combined with the padding $Z$ and the reference image $R$, and the result will be doenoted as `X_Z_R` in the code.  

After the image, separator, and padding are concatenated left-to-right (i.e., by column), the result $\textrm{concat}(X, Z, R)$ is padded with zero values on the right and on the bottom to a final size of $M_1 \cdot M_2.$  Let $\textrm{pad}(\textrm{concat}(X, Z, R), M_1, M_2)$ be the result.


### Fourier transform and normalization

The concatenated and padded image $\textrm{pad}(\textrm{cat}(X, Z, R), M_1, M_2)$ is next put through a discrete, two-dimensional fast Fourier transform (FFT).  The resulting complex values are then reduced to their magnitudes using the absolute value function and then squared.  The resulting variable $V$ is the FFT-transformed and squared values,
\begin{equation}
V = \left| \, \strut  \textrm{fft2}(\textrm{pad}(\textrm{concat}(X, Z, R), M_1, M_2)) \, \right|^2.
\end{equation}
Next, this value is normalized by dividing by the mean of the values to define the expected number of photons at each sensor in the padded, concatenated image,
\begin{equation}
\lambda = \frac{N_p}{\textrm{mean}(V)} \cdot V,
\end{equation}
where $N_p$ is the total number of photons observed, which is supplied as a constant.

### Beamstop

To avoid the coherent X-ray beam from melting the biomolecule and destroying the sensors, the low frequency elements are blocked with a beamstop.  This prevents the photons from getting through and result in artifical zero counts.

The beamstop has a configurable radius $r$, with zero values everywhere other than the 1 values in the the top left corner ($r \times r$ block), top right ($r \times r - 1$ block), lower left ($r-1 \times r$ block) and lower right ($r - 1 \times r - 1$ block).  We will let $\textrm{beamstop}_{i, j}$ be equal to 1 if position $(i, j)$ is stopped (i.e., blocked) and 0 otherwise.  When pieced together, that's a $2 \cdot r - 1$ square that's stopped.


### Poisson photon sampling

Finally, the actual number of photons at each position is generated as a Poisson process,
\begin{equation}
Y_{i, j} \sim
\begin{cases}
\textrm{Poisson}(\lambda_{i, j}) & \textrm{ if } \textrm{beamstop}(i, j) = 0
\\
\textrm{Poisson}(0) & \textrm{ if } \textrm{beamstop}(i, j) = 1.
\end{cases}
\end{equation}

This completes the definition of the sampling distribution (equivalently, the likelihood).  This representation is using the Poisson trick that reduces to the same probability mass function for $y$ as a multinomial with probabilities proportional to $V$ and a total of $N_p$ photons per receptor on average.  In general, suppose we have a multinomial of the form
\begin{equation}
\textrm{multinomial}\left( y \mid \theta, \textrm{sum}(y) \right),
\end{equation}
where, in our case,
\begin{equation}
\textrm{sum}(y) = N_p \cdot M_1 \cdot M_2
\end{equation}
and
\begin{equation}
\theta = N_p \cdot \frac{V \cdot \textrm{beamstop}}{\textrm{mean}(V \cdot \textrm{beamstop})}.
\end{equation}
Conditiononed on having observed $y,$ the same likelihood contribution up to a multiplicative constant is given by
\begin{equation}
\prod_{i = 1}^{M_1} \prod_{j = 1}^{M_2} \textrm{Poisson}(y_{i, j} \mid \theta_{i, j} \cdot \textrm{sum}(y)).
\end{equation}




# References {.unnumbered}

::: {#refs}
:::


# Appendicies {.unnumbered}

## A. Stan Program {#full-code .appendix .unnumbered}

```stan
{{< include holoml.stan >}}
```
