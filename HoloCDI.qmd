---
title: "Estimation and Sampling for Holographic Coherent Diffraction Imaging"
subtitle: "Low-photon phase retrieval with MLE, MAP, VB, and MCMC"
author:
  - name: Brian Ward
    corresponding: true
    email: bward@flatironinstitute.org
    url: https://brianward.dev/
    orcid: 0000-0002-9841-3342
    affiliations:
      - name: Flatiron Institute
        department: Center for Computational Mathematics
        url: https://www.simonsfoundation.org/flatiron/center-for-computational-mathematics/
  - name: Bob Carpenter
    email: bcarpenter@flatironinstitute.org
    url: https://bob-carpenter.github.io/
    orcid: 0000-0002-2433-9688
    affiliations:
      - name: Flatiron Institute
        department: Center for Computational Mathematics
        url: https://www.simonsfoundation.org/flatiron/center-for-computational-mathematics/
  - name: David Barmherzig
    email: dbarmherzig@flatironinstitute.org
    url: https://davidbar.org/
    orcid: 0000-0003-2466-981X
    affiliations:
      - name: Flatiron Institute
        department: Center for Computational Mathematics
        url: https://www.simonsfoundation.org/flatiron/center-for-computational-mathematics/
date: last-modified
date-modified: last-modified
description: |
#  This case study surveys several inference and evaluation schemes for holographic coherent diffraction imaging.
abstract: >+
  This case study provides a forward statistical model for holographic coherent diffraction imaging (Holo CDI) that generates a random photon flux from an image and explores several ways of solving the inverse problem (i.e., recovering the image from the measured flux). Maximum a posteriori estimates (MAP) and penalized maximum likelihood estimates (MLE) are provided by limited memory quasi-Newton optimization (L-BFGS).  Variational Bayes (VB) is implemented with Pathfinder.  Markov chain Monte Carlo (MCMC) is implemented with the no-U-turn sampler (NUTS).  Evaluations include log density, root mean square error (RMSE), and the image-specific measures structural similarity (SSIM) and and peak signal-to-noise ratio (PSNR).
keywords: [Stan, coherent-diffraction-imaging, statistics, bayesian, phase-retrieval, inverse-problem <br /> <br /> ]
citation:
  type: article-journal
  container-title: "Computo"
  doi: "xxxx"
  url: https://computo.sfds.asso.fr/template-computo-quarto
  publisher: "Société Française de Statistique"
  issn: "2824-7795"
bibliography: references.bib
github-user: WardBrian
repo: "holoml-in-stan-computo"
draft: true # set to false once the build is running
published: false # will be set to true once accepted
monobackgroundcolor: white
format:
  computo-html: default
  computo-pdf: default
jupyter: python3
---


# Introduction

Coherent diffraction imaging (CDI) is a technique for imaging nanoscale biomolecules such as macroviruses and proteins.  CDI involves exposing the object being imaged to a coherent beam of X-rays and measuring the diffracted photon flux at a sensor placed behind the object being imaged.  The diffraction pattern theoretically follows a Fourier transform, but only the squared magnitude is observed, which induces a phase-retrieval problem where the real and imaginary components of the complex variables must be inferred so that the process may be inverted to form an image.  Solving the inverse problem for CDI is highly challenging and typically lacks a unique solution [@barnett2020].

Holographic coherent diffraction imaging (HCDI) is a variant of CDI in which the specimen is placed some distance away from a known reference object, and the data observed is the pattern of  diffraction around both the specimen and the reference. The addition of a reference object reduces the inverse problem to a linear deconvolution problem which has a unique, closed-form solution in the idealized setting [@barmherzig2019].

In this note, we follow @barmherzig2022 in defining a forward statistical model that, given the image being reconstructed, defines the expected photon flux at the sensors in terms of the Fourier transform of the image and Poisson sampling of incident photons.  Starting from the model of @barmherzig2022, we add an intrinsic conditional autoregressive (ICAR) prior on the image which can be used to control smoothing of adjacent pixels [@besag1991].  We then log-odds (logit) transform the parameters of the model, which represent pixel intensity between 0 and 1, into unconstrained variables so the model has support on all of $\mathbb{R}^{M \times N}$ for an $M \times N$ pixel image. 

The forward model of holographic CDI defines a sampling distribution for the observed photon flux.  When combined with a prior, the forward model induces an inverse problem whereby we measure photon flux on a grid of sensors and infer the image that caused it.  We will explore the following standard algorithms for exact and approximate solution of the inverse problem.

* Penalized maxmum likelihood (MLE):  Solve with optimization, do not adjust for unconstraining transform.
* Maximum a posteriori (MAP): Solve with optimization, adjusting for the unconstraining transform.
* Variational Bayes (VB): Solve by minimizing Kullback-Leibler divergence from an approximate posterior to the true posterior and importance resampling.
* Markov chain Monte Carlo (MCMC): Solve by sampling from the posterior and averaging.

Computationally, we will code the models using Stan [@carpenter2017], a probabilistic programming language for expressing differentiable log densities.  All that is needed to solve the inverse problem with Stan is a program implementing the forward model's log density.  Stan uses derivative-based methods for optimization, sampling, and variational inference in order to scale with dimension.  It also contains a comprehensive suite of posterior analysis tools for summarizing fits to data.  

@barmherzig2022 used two different optimizers for maximum likelihood inference, conjugate gradient and trust regions. We will employ limited memory quasi-Newton optimization in the form of the limited-memory BFGS (L-BFGS) optimizer [@zhu1997], which tends to outperform both conjugate gradient and trust-region methods for relatively simple optimization problems in moderately high dimensions like this one.  If the model adjusts for the inverse log odds (logistic) transform, the optimizer produces a maximum a posteriori (MAP) estimate, whereas if the model does not adjust for the change of variables, the optimizer produces a penalized maximum likelihood estimate (MLE).

We employ two different Bayesian methods for solving the inverse problem, variational Bayes (VB) and Markov chain Monte Carlo (MCMC) sampling.  For MCMC, we employ the no-U-turn sampler (NUTS) [@hoffman2014], which is an adaptive form of Hamiltonian Monte Carlo (HMC).  With MCMC, the result is an exact posterior sampler for which estimation error goes to zero as sample size goes to infinity.  For variational Bayes, we employ Pathfinder [@zhang2022].  With VB, the results are only approximate, and in particular, posterior uncertainty tends to be systematically underestimated because we minimize KL-divergence from the approximation to the target distribution.

## Python boilerplate

Generating the data requires the standard Python numerical libraries `scipy.stats` and `numpy`, as well as the plotting library `matplotlib` to display results.  Stan is accessed through the `cmdstanpy` and `bridgestan` interfaces.  There is also code for converting RGB-coded images to grayscale.

```{python}
import cmdstanpy as csp
import bridgestan as bs
import numpy as np
import scipy as sp
import os
import matplotlib as mpl
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import plotnine as pn
import pandas as pd

# disable axes drawing, since we are showing images
# mpl.rc('axes.spines', top=False, bottom=False, left=False, right=False)
# mpl.rc('axes', facecolor='white')
# mpl.rc("xtick", bottom=False, labelbottom=False)
# mpl.rc("ytick", left=False, labelleft=False)

def rgb2gray(rgb):
    """Convert a nxmx3 RGB array to a grayscale nxm array.

    This function uses the same internal coefficients as MATLAB:
    https://www.mathworks.com/help/matlab/ref/rgb2gray.html
    """
    r, g, b = rgb[:, :, 0], rgb[:, :, 1], rgb[:, :, 2]
    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b

    return gray
```

# Test images

For simplicity of exposition, we will restrict attention to black and white digital images at $256 \times 256$ pixel resolution.  The pixel values will be represented with continuous values in $[0, 1]$, with $0$ being pure black and $1$ pure white. An image is thus an element $X \in [0, 1]^{256 \times 256}.$  Compute time to evaluate the log density and gradients scales linearly in time and memory for each image dimension, which means it scales quadratically in $N$ for an $N \times N$ image.

## Primary test image

We will concentrate on the target image of a [mimivirus](https://en.wikipedia.org/wiki/Mimivirus), a type of [giant virus](https://en.wikipedia.org/wiki/Giant_virus). It is giant in the sense that its genome is over one million base pairs in length.  The mimivirus is structured as an icosahedral capsid of approximately 400nm with filaments extending another 100nm around the capsid.  An image is shown in @fig-mimivirus.

```{python}
#| fig-cap: Image of a mimivirus.  The capsid is an icosahedron 400nm across with 100nm filaments. Image copyright 2008 by E. Ghigo J. Kartenbeck, P. Lien, L. Pelkmans, C. Capo, J.L. Mege, and D. Raoult D. 2008, and distributed under the [CC BY 2.5](https://creativecommons.org/licenses/by/2.5) license.
#| label: fig-mimivirus
#| fig-align: center
#| fig-pos: 't'

X_src = rgb2gray(mpimg.imread('img/mimivirus.png'))
plt.figure(figsize=(3, 3))
plt.imshow(X_src, cmap='gray', vmin=0, vmax=1)
plt.xticks([0, 127, 255], [1, 128, 256])
plt.yticks([0, 127, 255], [1, 128, 256])
plt.tight_layout()
plt.show()
```

## Secondary test images

In addition to the mimivirus image, we will also simulate and reconstruct the ten images show in @fig-test_img.

```{python}
#| fig-cap: Images form the USC/SIPI test repository.  First row, left to right) female [Bell Labs?], house, tree, jelly beans, moon surface.  Second row, left to right) aerial, airplane, clock, resolution chart, chemical plant.
#| label: fig-test_img
#| fig-align: center
#| fig-pos: 't'

img_dir = 'img/usc-sipi'
image_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])
fig, axs = plt.subplots(2, 5, figsize=(20, 8))
axs = axs.ravel()
for i, ax in enumerate(axs):
    img_path = os.path.join(img_dir, image_files[i])
    gray_img = mpimg.imread(img_path)
    ax.imshow(gray_img, cmap='gray')
    ax.axis('off')
plt.tight_layout()
plt.show()
```


# The generative model

## Image and photon sensor representations

An image is represented by a random matrix of continuous pixel values between 0 and 1,
$X \in [0, 1]^{N \times N}$.  Typically we will be using a representation that excludes the endpoints other than through numerical underflow or rounding.  The photons measured at the sensor are represented by a random matrix of discrete counts for the sensor grid, $Y \in \mathbb{N}^{M_1 \times M_2}.$

## Unconstrained representation and change of variables correction

It is more convenient for both sampling and optimizatio to work over an unconstrained parameterization with support in all of $\mathbb{R}^D$ for some $D.$  As an alternative to the constrained image representation $X$, consider the smooth, bijective, and strictly monotonic log odds transform applied elementwise,
\begin{equation}
U = \textrm{logit}(X),
\end{equation}
where
\begin{equation}
\textrm{logit}(v) = \log \left( \frac{v}{1 - v} \right)
\qquad
\textrm{logit}^{-1}(u) = \frac{1}{1 + \exp(-u)}.
\end{equation}
If $p_X$ has support on $(0, 1)^{N \times N},$ then $p_U$ has support on all of $\mathbb{R}^{N \times N}.$  The density of $U$ is given by the change of variables formula,
\begin{equation}
p_U(u) = p_X(\textrm{logit}^{-1}(u)) \cdot \left| \prod_{i, j} (\textrm{logit}^{-1})'(u_{i,j}) \right|,
\end{equation}
where the derivative of the logistic sigmoid $\textrm{logit}^{-1}$ is
\begin{equation}
\left(\textrm{logit}^{-1}\right)'(u)
= \textrm{logit}^{-1}(u) \cdot \left( 1 - \textrm{logit}^{-1}(u) \right).
\end{equation}
On the log scale,
\begin{equation}
\log p_U(u) = \log p_X(\textrm{logit}^{-1}(u))
 + \sum_{i, j} \log \left( (\textrm{logit}^{-1})'(u_{i,j}) \right).
\end{equation}



## Generative process overview

The first step is to generate an image from the prior,
\begin{equation}
x \sim p_X\,(\cdot).
\end{equation}
Then, given the image $x$, the second and final step is to generate the photons observed at the sensors $y$ conditoned on the image $x,$ using the sampling distribution,
\begin{equation}
y \sim p_{Y \mid X}\,(\cdot \mid x).
\end{equation}


## The prior

### Uniform prior is possible

Because the space $[0, 1]^{256 \times 256}$ has finite hypervolume, it would be possible to use a uniform prior over pixel values, $p_X(x) \propto 1.$  At the other extreme, it would also be possible to introduce a meaningful natural image prior [@kadkhodaie2021], a promising modern approach to which is based on diffusion models [@graikos2022,@fei2023].

### ICAR prior in constrained space

Instead of a uniform prior or natural image prior, we will use a simple intrinsic conditional autoregressive (ICAR) prior [@besag1991], which penalizes differences between adjacent pixels according to a scale $\sigma > 0.$ These priors were originally developed for image denoising. The ICAR prior is defined by
\begin{equation}
p_X(x) \propto \prod_{(m, n) \sim (m', n')} \textrm{normal}(x_{m,n} \mid x_{m',n'}, \ \sigma),
\end{equation}
where $\sim$ is the adjacency relationship defined to be unique so that  $(m, n) \sim (m + 1, n)$ for $m < 256$ and $(m, n) \sim (m, n + 1)$ for $n < 256$, and where $\sigma$ is the scale of the normal distribution.  This density $p_X$ can, in theory, be normalized within the space $[0, 1]^{256 \times 256},$ but that is not going to be necessary for any of the inference methods we use.  It would also be possible to smooth along the diagonals by including an adjacency relation $(m, n) \sim (m + 1, n + 1)$ for $m, n < 256,$ but we do not do evaluate that option here.

Because we will be sampling using the unconstrained space $U,$ we apply the change of variables from $X$ to derive the density in the unconstrained space,
\begin{align}
p_U(u) \propto { } &
\prod_{(m, n) \sim (m', n')}
\textrm{normal}\!\left(\textrm{logit}^{-1}(u_{m,n}) \mid \textrm{logit}^{-1}(u_{m',n'}), \ \sigma\right)
\\
& 
\hspace*{1em} \cdot \prod_{m, n} \textrm{logit}^{-1}(u_{m, n}) \cdot (1 - \textrm{logit}^{-1}(u_{m,n})).
\end{align}

### ICAR prior in unconstrained space

An ICAR prior may also be placed directly on the unconstrained values,
\begin{equation}
p_U(u) \propto \prod_{(m, n) \sim (m', n')} \textrm{normal}(u_{m,n} \mid u_{m',n'}, \ \tau),
\end{equation}
with $\tau$ being the scale of variation of the unconstrained parameters.  As before, $X = \textrm{logit}^{-1}(u)$ recovers the pixel values for use downstream in the model.  Conveniently, when parameterized this way, the Jacobian adjustment is not necessary because a distribution is not being placed on the constrained parameters.  This approach would not be feasible for natural image priors placed directly on $X$.

## The sampling distribution

The sampling distribution proceeds in several stages.  First, the image is combined with a separator and a reference image and then padded.  The result defines the shape of the sensor grid.  Next, a discrete Fourier transform is applied to the padded and separated image and reference, the squared absolute values of which determine the proportion of photons that are expected to arrive at the specified sensor.  Then a beamstop is applied to the model where it is applied in practice to prevent damage to the sample or sensors.  Finally, the expected proportions are normalized for the intensity of the beam and then the photons are generated according to a simple Poisson process.

### The reference image

For the experiments here, we use an idealized optimal reference image $R$, the uniformly redundant array (URA) reference [@fenimore1978]; see @fig-ura.  The URA has been shown to be an optimal reference image for this kind of work.   We have just included the reference array as a file, but it can also be generated programatically using a Python package such as [cappy](https://github.com/bpops/cappy).   The URA is black and white (i.e., no intermediate gray values), and it has the same dimensions ($256 \times 256$) as the image being inferred. It will be supplied as data, so that other references can be swapped in depending on the actual reference image being used (e.g., a square cutout, a pinhole, or no reference at all).

```{python}
#| fig-cap: Uniformly redundant array. This reference image is optimal for identification through phase retrieval. Other reference images may be used with the same code by swapping the reference data file.
#| label: fig-ura
#| fig-align: center
#| fig-pos: 't'

R = np.loadtxt('img/URA.csv', delimiter=",", dtype=int)
plt.figure(figsize=(3, 3))
plt.imshow(R, cmap='gray')
plt.xticks([0, 127, 255], [1, 128, 256])
plt.yticks([0, 127, 255], [1, 128, 256])
plt.tight_layout()
plt.show()
```

### The seperator and reference image

There is also a purely black separator of the same size as the image, which we will use $Z$ to denote because it is full of zero values.  Like the reference pattern, this separator is typically chosen to be the same size as the specimen, so that its width $d$ is equal to $N,$ the pixel width of the image.  The image $X$ will be combined with the padding $Z$ and the reference image $R$, and the result will be doenoted as `X_Z_R` in the code.  

After the image, separator, and padding are concatenated left-to-right (i.e., by column), the result $\textrm{concat}(X, Z, R)$ is padded with zero values on the right and on the bottom to a final size of $M_1 \cdot M_2.$  Let $\textrm{pad}(\textrm{concat}(X, Z, R), M_1, M_2)$ be the result.

```{python}
#| fig-cap: Source image, zero separator, and reference image concatenated.
#| label: fig-separator
#| fig-align: center
#| fig-pos: 't'

N = np.shape(X_src)[0]
d = N
X0R = np.concatenate([X_src, np.zeros((N, d)), R], axis=1)
plt.figure(figsize=(9, 3))
plt.imshow(X0R, cmap='gray')
plt.xticks([0, 255, 511, 767], [1, 256, 512, 768])
plt.yticks([0, 127, 255], [1, 128, 256])
plt.tight_layout()
plt.show()
```

It is this combination of image, separator, and reference are the ones subjected to a beam of X-rays.


### Padding

The original image, separator and reference image are next padded to the right and below up to a size of $M_1 \times M_2$.  Sensors are placed at each position in the padded matrix.  The padding has the effect of interpolating in image space.  Let
\begin{equation}
P = \textrm{pad}(\textrm{cat}(X^{\textrm{src}}, Z, R), M_1, M_2)
\end{equation}
be the padded version of the image, separator, and reference image, where $X^{\textrm{src}}$ is the source image, $Z$ is a zero matrix, and $R$ is the reference image.  The padding is taken to be twice the size of the material imaged, so that $M_1 = 6 \cdot 256$ and $M_2 = 2 \cdot 256,$ which brings the total number of sensors to 786,432, just shy of one megapixel. 



### Fourier transform shift

Most fast Fourier transform (FFT) interfaces, including those in Stan, Matlab, and Python, return a shifted result with positive and negative values.  Specifically, values increase in frequency until $N,$ at which point a mirror image copy with negative indexes is appended to the end.  This produces the following order of indexes.
\begin{equation}
0, 1, \ldots, N, -N, -N + 1, \ldots, -1.
\end{equation}
To get centered values, these are circularly shifted until the lowest frequency, represented at position 0, is in the middle, 
\begin{equation}
-N, -N + 1, \ldots -1, 0, 1, \ldots, N-1, N.
\end{equation}
With a two dimensional FFT, this happens twice, once for the rows and once for the columns.  This results in a horizontal and vertical shift, which is why the `fftshift` operation swaps the diagonal blocks.  This can be illustrated with the following code, which creates a matrix of distinct entries, shifts it, then inverse shifts it back.  Assuming $A, B, C, D$ are matrices with conforming shapes (for building one big matrix), then
\begin{equation}
\textrm{fftshift}\left(
\begin{bmatrix}
A & B
\\
C & D
\end{bmatrix}
\right)
=
\begin{bmatrix}
D & C
\\
B & A
\end{bmatrix}
\end{equation}
The elements are swapped across the diagonals, or equivalently, cyclically rotated in the horizontal direction, then in the vertical direction.  Consider the following concrete example matrix.

```{python}
K = 4
A = np.ones((K, K), dtype=int)
for i in range(K):
    for j in range(K):
    	A[i, j] = 10 * i + j
print(f"A =\n{A}\n")
A_shift = np.fft.fftshift(A)
```
Shifting it produces the following shifted matrix, where the center elements move to the corners and vice-versa.
```{python}
print(f"fftshift(A) =\n{A_shift}\n")
```
Shifting back returns the original matrix.
```{python}
A_shift_ishift = np.fft.ifftshift(A_shift)
print(f"ifftshift(fftshift(A)) =\n{A_shift_ishift}")
```

### Fourier transform

The concatenated and padded image $P$ is next put through a discrete, two-dimensional fast Fourier transform (FFT), which produces a complex matix of values.  These complex values, which have real and imaginary components, are then reduced to their real absolute value (aka magnitude).  The complex absolute value is the hypoteneuse of their real and imaginary components, but the angle, or phase, is lost.  This is why the problem of inferring the image from its Fourier transform is known as a phase-retrieval problem.  The absolute value is then squared, resulting in a matrix of values that is proportional to the expected intensity of sensor readings given the image, separator, reference, and padding,
\begin{equation}
V = \big| \, \textrm{fft2}(P) \, \big|^2.
\end{equation}
$P$ has shape $512 \times 1536,$ so the two-dimensional FFT requires 1536 one-dimensional FFTs of size 512 and 512 one-dimensional FFTs of size 1536.  @fig-fftout shows a visualization of $V$, after the FFT shift to center the low frequencies.  The expected photon flux defines a lovely tartan pattern with horizontal and vertical reflection symmetries and a starburst in the center, which indicates the high expected photon flux in the low frequencies (which have been shifted to the center).

```{python}
#| fig-cap: Squared magnitudes of the FFT of the concatenated and padded image.  The low frequencies have been shifted to the center and the reflective symmetries show how the two-dimensional FFT has horizontal and vertical reflection symmetry. The bright spot in the middle shows the high expected photon flux in the low frequencies.  The plot is scaled to $\log (1 + V)$ in order to display the pattern.
#| label: fig-fftout
#| fig-align: center
#| fig-pos: 't'

M1 = 256 * 2
M2 = 256 * 3 * 2
V = np.abs(np.fft.fft2(X0R, s=(M1, M2)))**2
log1pV = np.fft.fftshift(np.log1p(V))
plt.figure(figsize=(9,3))
plt.imshow(log1pV, cmap="viridis")
plt.xticks([0, 767, 1535], [1, 768, 1536])
plt.yticks([0, 255, 511], [1, 256, 512])
plt.tight_layout()
plt.show()
```



### Beamstop

To prevent the coherent X-ray beam from destroying the sensors with high photon flux in the low frequencies, the frequency sensors are blocked with a beamstop.  This blockage means zero photons arrive at sensors in the stopped positions.  The image must be reconstructed from only the remaining sensors.

The beamstop has a configurable radius $r$ and occludes an $(2 \cdot r - 1) \times (2 \cdot r - 1)$ square of censors.  These are placed at the low frequencies, which will be the center of the image when it is centered around zero. After the FFT shift is applied, the stopped regions move to the four corners (see the example in the previous section).

The shifted beamstop can be directly encoded as a matrix with zero values everywhere other than the 1 values in the the top left corner ($r \times r$ block), top right ($r \times r - 1$ block), lower left ($r-1 \times r$ block) and lower right ($r - 1 \times r - 1$ block).  We will let $\textrm{beamstop}_{i, j}$ be equal to 1 if position $(i, j)$ is stopped (i.e., blocked) and 0 otherwise.  When pieced together, that's a $(2 \cdot r - 1) \times (2 \cdot r - 1)$ square that's stopped.  A plot of the beamstop is shown in @fig-beamstop.

```{python}
#| fig-cap: Illustration of beamstop before the shifting.  The solid blue rectangles occlude the low frequencies in the corners, so that photon flux is not measured at those positions.
#| label: fig-beamstop
#| fig-align: center
#| fig-pos: 't'

plot = (
  pn.ggplot()
  + pn.geom_segment(pn.aes(x=0, y=0, xend=0, yend=4), color="black")
  + pn.geom_segment(pn.aes(x=0, y=4, xend=12, yend=4), color="black")
  + pn.geom_segment(pn.aes(x=12, y=4, xend=12, yend=0), color="black")
  + pn.geom_segment(pn.aes(x=12, y=0, xend=0, yend=0), color="black")
  # lower left
  + pn.geom_segment(pn.aes(x=0, y=0, xend=0, yend=1), color="blue")
  + pn.geom_segment(pn.aes(x=0, y=1, xend=3, yend=1), color="blue")
  + pn.geom_segment(pn.aes(x=3, y=1, xend=3, yend=0), color="blue")
  + pn.geom_segment(pn.aes(x=3, y=0, xend=0, yend=0), color="blue")
  + pn.geom_rect(pn.aes(xmin=0, xmax=3, ymin=0, ymax=1), fill='blue', alpha=0.5)
  # lower right
  + pn.geom_segment(pn.aes(x=9, y=0, xend=9, yend=1), color="blue")
  + pn.geom_segment(pn.aes(x=9, y=1, xend=12, yend=1), color="blue")
  + pn.geom_segment(pn.aes(x=12, y=1, xend=12, yend=0), color="blue")
  + pn.geom_segment(pn.aes(x=12, y=0, xend=9, yend=0), color="blue")
  + pn.geom_rect(pn.aes(xmin=9, xmax=12, ymin=0, ymax=1), fill='blue', alpha=0.5)
  # upper left
  + pn.geom_segment(pn.aes(x=0, y=3, xend=0, yend=4), color="blue")
  + pn.geom_segment(pn.aes(x=0, y=4, xend=3, yend=4), color="blue")
  + pn.geom_segment(pn.aes(x=3, y=4, xend=3, yend=3), color="blue")
  + pn.geom_segment(pn.aes(x=3, y=3, xend=0, yend=3), color="blue")
  + pn.geom_rect(pn.aes(xmin=0, xmax=3, ymin=3, ymax=4), fill='blue', alpha=0.5)
  # upper right
  + pn.geom_segment(pn.aes(x=9, y=3, xend=9, yend=4), color="blue")
  + pn.geom_segment(pn.aes(x=9, y=4, xend=12, yend=4), color="blue")
  + pn.geom_segment(pn.aes(x=12, y=4, xend=12, yend=3), color="blue")
  + pn.geom_segment(pn.aes(x=12, y=3, xend=9, yend=3), color="blue")
  + pn.geom_rect(pn.aes(xmin=9, xmax=12, ymin=3, ymax=4), fill='blue', alpha=0.5)
  + pn.scale_x_continuous(breaks=[0, 3, 9, 12],
                          labels=["1", "r", "M1-r+2", "M1"])
  + pn.scale_y_continuous(breaks=[0, 1, 3, 4],
                          labels=["1", "r", "M2-r+2", "M2"])
  + pn.theme(figure_size=(6, 2),
             plot_background=pn.element_rect(fill="white"),
             panel_background=pn.element_rect(fill="white"),
             axis_title=pn.element_blank(),
	     axis_text_x=pn.element_text(size=10, color="black", angle=0),
             axis_text_y=pn.element_text(size=10, color="black"),
	     axis_ticks=pn.element_blank(),
	     axis_line=pn.element_blank(),
             # axis_ticks_length_major=5,
	     # axis_line=pn.element_line(color="black"),
             panel_grid=pn.element_blank())
)
plot
```

### Normalization

Let $N_p$ be the number of photons expected per sensor.  This is used like an exposure value in a Poisson model to set the scale of the expected photon counts, which are given by the vector $\lambda \in (0, \infty)^{M_1 \times M_2},$ defined by
\begin{equation}
\lambda = \frac{N_p}{\textrm{mean}(V)} \cdot V.
\end{equation}

### Poisson photon sampling

Finally, the number of photons observed at each sensor in the grid is generated as a Poisson distribution, based on the expected photon flux  $\lambda,$
\begin{equation}
Y_{i, j} \sim
\begin{cases}
\textrm{Poisson}(\lambda_{i, j}) & \textrm{ if } \textrm{beamstop}(i, j) = 0
\\[4pt]
\textrm{Poisson}(0) & \textrm{ if } \textrm{beamstop}(i, j) = 1.
\end{cases}
\end{equation}

A visualization of simulated $Y$ is shown (after being shifted) in @fig-simy.  The beamstop radius is set to $r = 13,$ so there is a $25 \times 25$ pixel square of sensors occluded by the beamstop.

```{python}
#| fig-cap: Sampled value of $Y$, which is distributed Poisson with rate $\lambda$.  The effect of the beamstop can be seen in the $25 \times 25$ pixel black square in the center, indicating a flux of zero. The scaled matrix $\log(1 + Y)$ is shown.
#| label: fig-simy
#| fig-align: center
#| fig-pos: 't'

r = 13
shifted_beamstop = np.ones((M1, M2))  # 0 values are occluded
shifted_beamstop[M1 // 2 - r + 1: M1 // 2 + r,
                 M2 // 2 - r + 1: M2 // 2 + r] = 0
beamstop = np.fft.ifftshift(shifted_beamstop)

N_p = 1  # expected number of photons per sensor
lambbda = N_p / V.mean() * V
preY = sp.stats.poisson.rvs(lambbda, random_state=1234)
Y = preY * beamstop

log1pY = np.log1p(Y)
plt.figure(figsize=(9,3))
plt.imshow(np.log1p(np.fft.fftshift(Y)), cmap="viridis")
plt.xticks([0, 767, 1535], [1, 768, 1536])
plt.yticks([0, 255, 511], [1, 256, 512])
plt.tight_layout()
plt.show()
```

This completes the definition of the likelihood and with the prior, the forward model for simulating data.

# References {.unnumbered}

::: {#refs}
:::


# Appendicies {.unnumbered}

## A. Stan Program {#full-code .appendix .unnumbered}

```stan
{{< include stan/holoCDI.stan >}}
```
